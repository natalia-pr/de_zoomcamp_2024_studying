{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrTFv5nPClXh"
   },
   "source": [
    "# **Homework**: Data talks club data engineering zoomcamp Data loading workshop\n",
    "\n",
    "Hello folks, let's practice what we learned - Loading data with the best practices of data engineering.\n",
    "\n",
    "Here are the exercises we will do\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLF4iXf-NR7t"
   },
   "source": [
    "# 1. Use a generator\n",
    "\n",
    "Remember the concept of generator? Let's practice using them to futher our understanding of how they work.\n",
    "\n",
    "Let's define a generator and then run it as practice.\n",
    "\n",
    "**Answer the following questions:**\n",
    "\n",
    "- **Question 1: What is the sum of the outputs of the generator for limit = 5?**\n",
    "- **Question 2: What is the 13th number yielded**\n",
    "\n",
    "I suggest practicing these questions without GPT as the purpose is to further your learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLng-bDJN4jf",
    "outputId": "547683cb-5f56-4815-a903-d0d9578eb1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n",
      "1.7320508075688772\n",
      "2.0\n",
      "2.23606797749979\n",
      "SUM:  8.382332347441762\n",
      "1.0\n",
      "1.4142135623730951\n",
      "1.7320508075688772\n",
      "2.0\n",
      "2.23606797749979\n",
      "2.449489742783178\n",
      "2.6457513110645907\n",
      "2.8284271247461903\n",
      "3.0\n",
      "3.1622776601683795\n",
      "3.3166247903554\n",
      "3.4641016151377544\n",
      "3.605551275463989\n"
     ]
    }
   ],
   "source": [
    "def square_root_generator(limit):\n",
    "    n = 1\n",
    "    while n <= limit:\n",
    "        yield n ** 0.5\n",
    "        n += 1\n",
    "\n",
    "# Sum 0f 5 values:\n",
    "limit = 5\n",
    "generator = square_root_generator(limit)\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for sqrt_value in generator:\n",
    "    print(sqrt_value)\n",
    "    sum += sqrt_value\n",
    "\n",
    "print(\"SUM: \", sum)\n",
    "\n",
    "\n",
    "# 13th value:\n",
    "limit = 13\n",
    "generator = square_root_generator(limit)\n",
    "\n",
    "for sqrt_value in generator:\n",
    "    print(sqrt_value)\n",
    "    sum += sqrt_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbe3q55zN43j"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjWhILzGJMpK"
   },
   "source": [
    "# 2. Append a generator to a table with existing data\n",
    "\n",
    "\n",
    "Below you have 2 generators. You will be tasked to load them to duckdb and answer some questions from the data\n",
    "\n",
    "1. Load the first generator and calculate the sum of ages of all people. Make sure to only load it once.\n",
    "2. Append the second generator to the same table as the first.\n",
    "3. **After correctly appending the data, calculate the sum of all ages of people.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MoaQcdLBEk6",
    "outputId": "d2b93dc1-d83f-44ea-aeff-fdf51d75f7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 1, 'Name': 'Person_1', 'Age': 26, 'City': 'City_A'}\n",
      "{'ID': 2, 'Name': 'Person_2', 'Age': 27, 'City': 'City_A'}\n",
      "{'ID': 3, 'Name': 'Person_3', 'Age': 28, 'City': 'City_A'}\n",
      "{'ID': 4, 'Name': 'Person_4', 'Age': 29, 'City': 'City_A'}\n",
      "{'ID': 5, 'Name': 'Person_5', 'Age': 30, 'City': 'City_A'}\n",
      "{'ID': 3, 'Name': 'Person_3', 'Age': 33, 'City': 'City_B', 'Occupation': 'Job_3'}\n",
      "{'ID': 4, 'Name': 'Person_4', 'Age': 34, 'City': 'City_B', 'Occupation': 'Job_4'}\n",
      "{'ID': 5, 'Name': 'Person_5', 'Age': 35, 'City': 'City_B', 'Occupation': 'Job_5'}\n",
      "{'ID': 6, 'Name': 'Person_6', 'Age': 36, 'City': 'City_B', 'Occupation': 'Job_6'}\n",
      "{'ID': 7, 'Name': 'Person_7', 'Age': 37, 'City': 'City_B', 'Occupation': 'Job_7'}\n",
      "{'ID': 8, 'Name': 'Person_8', 'Age': 38, 'City': 'City_B', 'Occupation': 'Job_8'}\n"
     ]
    }
   ],
   "source": [
    "def people_1():\n",
    "    for i in range(1, 6):\n",
    "        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 25 + i, \"City\": \"City_A\"}\n",
    "\n",
    "for person in people_1():\n",
    "    print(person)\n",
    "\n",
    "\n",
    "def people_2():\n",
    "    for i in range(3, 9):\n",
    "        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 30 + i, \"City\": \"City_B\", \"Occupation\": f\"Job_{i}\"}\n",
    "\n",
    "\n",
    "for person in people_2():\n",
    "    print(person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at stage sync with exception:\n\n<class 'dlt.destinations.exceptions.DestinationConnectionError'>\nConnection with DuckDbSqlClient to dataset name generators failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Could not set lock on file \"/Users/nataliaprudnikova/GitHub/de_zoomcamp_2024_studying/workshop_dlt/dlt_ipykernel_launcher.duckdb\": Resource temporarily unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/destinations/sql_client.py:265\u001b[0m, in \u001b[0;36mraise_open_connection_error.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/destinations/impl/duckdb/sql_client.py:55\u001b[0m, in \u001b[0;36mDuckDbSqlClient.open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@raise_open_connection_error\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mDuckDBPyConnection:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mborrow_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# TODO: apply config settings from credentials\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/destinations/impl/duckdb/configuration.py:40\u001b[0m, in \u001b[0;36mDuckDbBaseCredentials.borrow_conn\u001b[0;34m(self, read_only)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_conn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn \u001b[38;5;241m=\u001b[39m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_owner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mIOException\u001b[0m: IO Error: Could not set lock on file \"/Users/nataliaprudnikova/GitHub/de_zoomcamp_2024_studying/workshop_dlt/dlt_ipykernel_launcher.duckdb\": Resource temporarily unavailable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDestinationConnectionError\u001b[0m                Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:659\u001b[0m, in \u001b[0;36mPipeline.sync_destination\u001b[0;34m(self, destination, staging, dataset_name)\u001b[0m\n\u001b[1;32m    658\u001b[0m restored_schemas: Sequence[Schema] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m remote_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_state_from_destination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# if remote state is newer or same\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# print(f'REMOTE STATE: {(remote_state or {}).get(\"_state_version\")} >= {state[\"_state_version\"]}')\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# TODO: check if remote_state[\"_state_version\"] is not in 10 recent version. then we know remote is newer.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:1371\u001b[0m, in \u001b[0;36mPipeline._restore_state_from_destination\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     schema \u001b[38;5;241m=\u001b[39m Schema(schema_name)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_destination_clients(schema)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mas\u001b[39;00m job_client:\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job_client, WithStateSync):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/destinations/job_client_impl.py:296\u001b[0m, in \u001b[0;36mSqlJobClientBase.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqlJobClientBase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/destinations/sql_client.py:267\u001b[0m, in \u001b[0;36mraise_open_connection_error.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DestinationConnectionError(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name, \u001b[38;5;28mstr\u001b[39m(ex), ex)\n",
      "\u001b[0;31mDestinationConnectionError\u001b[0m: Connection with DuckDbSqlClient to dataset name generators failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Could not set lock on file \"/Users/nataliaprudnikova/GitHub/de_zoomcamp_2024_studying/workshop_dlt/dlt_ipykernel_launcher.duckdb\": Resource temporarily unavailable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m generators_pipeline \u001b[38;5;241m=\u001b[39m dlt\u001b[38;5;241m.\u001b[39mpipeline(destination\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduckdb\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerators\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # we can load any generator to a table at the pipeline destnation as follows:\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mgenerators_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeople_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeople\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# # the outcome metadata is returned by the load and we can inspect it by printing it.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(info)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:193\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    191\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:238\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[1;32m    236\u001b[0m         ConfigSectionContext(pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections)\n\u001b[1;32m    237\u001b[0m     ):\n\u001b[0;32m--> 238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, schema_contract)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# sync state with destination\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrestore_from_destination\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_refresh\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_restored\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestination \u001b[38;5;129;01mor\u001b[39;00m destination)\n\u001b[1;32m    599\u001b[0m ):\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_destination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;66;03m# sync only once\u001b[39;00m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:164\u001b[0m, in \u001b[0;36mwith_schemas_sync.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlive_schemas:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# refresh live schemas in storage or import schema path\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mcommit_live_schema(name)\n\u001b[0;32m--> 164\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# save modified live schemas\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlive_schemas:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/dlt/pipeline/pipeline.py:730\u001b[0m, in \u001b[0;36mPipeline.sync_destination\u001b[0;34m(self, destination, staging, dataset_name)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_state(state)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msync\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, ex, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m: Pipeline execution failed at stage sync with exception:\n\n<class 'dlt.destinations.exceptions.DestinationConnectionError'>\nConnection with DuckDbSqlClient to dataset name generators failed. Please check if you configured the credentials at all and provided the right credentials values. You can be also denied access or your internet connection may be down. The actual reason given is: IO Error: Could not set lock on file \"/Users/nataliaprudnikova/GitHub/de_zoomcamp_2024_studying/workshop_dlt/dlt_ipykernel_launcher.duckdb\": Resource temporarily unavailable"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "# # define the connection to load to.\n",
    "# # We now use duckdb, but you can switch to Bigquery later\n",
    "generators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators')\n",
    "\n",
    "\n",
    "# # we can load any generator to a table at the pipeline destnation as follows:\n",
    "info = generators_pipeline.run(people_1(),\n",
    "\t\t\t\t\t\t\t\t\t\ttable_name=\"people\",\n",
    "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\")\n",
    "\n",
    "# # the outcome metadata is returned by the load and we can inspect it by printing it.\n",
    "# print(info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtdTIm4fvQCN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY4cFAWOSwN1"
   },
   "source": [
    "# 3. Merge a generator\n",
    "\n",
    "Re-use the generators from Exercise 2.\n",
    "\n",
    "A table's primary key needs to be created from the start, so load your data to a new table with primary key ID.\n",
    "\n",
    "Load your first generator first, and then load the second one with merge. Since they have overlapping IDs, some of the records from the first load should be replaced by the ones from the second load.\n",
    "\n",
    "After loading, you should have a total of 8 records, and ID 3 should have age 33.\n",
    "\n",
    "Question: **Calculate the sum of ages of all the people loaded as described above.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKB2GTB9oVjr"
   },
   "source": [
    "# Solution: First make sure that the following modules are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTVvtyqrfVNq"
   },
   "outputs": [],
   "source": [
    "#Install the dependencies\n",
    "%%capture\n",
    "!pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2-PRBAkGC2K"
   },
   "outputs": [],
   "source": [
    "# to do: homework :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoTJu4kbGG0z"
   },
   "source": [
    "Questions? difficulties? We are here to help.\n",
    "- DTC data engineering course channel: https://datatalks-club.slack.com/archives/C01FABYF2RG\n",
    "- dlt's DTC cohort channel: https://dlthub-community.slack.com/archives/C06GAEX2VNX"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
